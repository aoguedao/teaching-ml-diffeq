{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System of Equations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's motivate this section with  the __Lotka-Volterra equations__, also known as the predator-prey equations, are a pair of first-order nonlinear differential equations, frequently used to describe the dynamics of biological systems in which two species interact. The populations change through time according to the pair of equations:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\dfrac{dx}{dt} = \\alpha x - \\beta xy \\\\\n",
    "\\dfrac{dy}{dt} = -\\gamma y + \\delta xy\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where\n",
    "- $x$ is the number of prey;\n",
    "- $y$ is the number of some predator;\n",
    "- ${\\tfrac {dy}{dt}}$ and ${\\tfrac {dx}{dt}}$ represent the instantaneous growth rates of the two populations;\n",
    "- $t$ represents time;\n",
    "- $\\alpha$, $\\beta$, $\\gamma$ and $\\delta$ are positive real parameters describing the interaction of the two species."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physics-Informed Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For years mathematicians and physicists are trying to model the world with differential equations. However, since the advent of techniques such as machine learning, neural networks and deep learning together with greater computing power, community has speculated that we could learn automatically (algorithms) anything with a enough amount of data. However, it seems this is not really true.\n",
    "\n",
    "Philosophical Question: Could have a _machine_ discovered Newton's laws?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In 2019, Raissi, Perdikaris and Karniadakis introduced Physics-Informed Neural Networks (PINNs), neural networks that are trained to solve supervised learning tasks while respecting any given law of physics described by general nonlinear partial differential equations ([source](https://www.sciencedirect.com/science/article/pii/S0021999118307125)). PINNs are nowadays used to solve PDEs, fractional equations, and integral-differential equations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PINNs approximate PDE solutions by training a neural network to minimize a loss function, including:\n",
    "\n",
    "* Initial and boundary conditions along the space-time domainâ€™s boundary\n",
    "* PDE residual at selected points in the domain.\n",
    "\n",
    "If you want to do a simplified analogy, initial and boundary conditions points will be an usual training dataset, but also it is necessary to embed physical laws (PDE) into the neural network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PINNs can solve differential equations expressed, in the most general form, like:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{F}(u(z); \\lambda) &= f(z) \\quad z \\text{ in } \\Omega \\\\\n",
    "\\mathcal{B}(u(z)) &= g(z) \\quad z \\text{ in } \\partial \\Omega\n",
    "\\end{align*}\n",
    "$$ \n",
    "\n",
    "defined on the domain $\\Omega \\subset \\mathbb{R}^d$ with the boundary $\\partial \\Omega$. Where\n",
    "- $z := (x_1, x_2, \\ldots, t)^\\top$ indicated the space-time coordinate vector,\n",
    "- $u$ the unknown function,\n",
    "- $\\lambda$ the parameters related to the physics,\n",
    "- $\\mathcal{F}$ the non-linear differential operator,\n",
    "- $f$ the function identifying the data of the problem,\n",
    "- $\\mathcal{B}$ the operator indicating arbitrary initial or boundary conditions, and\n",
    "- $g$ the boundary function.\n",
    "\n",
    "In the PINN methodology, $u(z)$ is computationally predicted by a NN, parametrized by a set of parameters $\\theta$, giving rise to an approximation\n",
    "$$\n",
    "\\hat{u}_\\theta(z) \\approx u(z)\n",
    "$$\n",
    "\n",
    "The optimization problem we want to deal with it is\n",
    "\n",
    "$$\n",
    "\\min_\\theta \\; \\omega_\\mathcal{F} \\mathcal{L}_\\mathcal{F}(\\theta) + \\omega_\\mathcal{B} \\mathcal{L}_\\mathcal{B}(\\theta) + \\omega_{\\text{data}} \\mathcal{L}_{\\text{data}}(\\theta)\n",
    "$$\n",
    "\n",
    "this is three weighted loss functions, each one depending on\n",
    "- $\\mathcal{L}_\\mathcal{F}$, differential equation, \n",
    "- $\\mathcal{L}_\\mathcal{B}$, boundary conditions, and\n",
    "- $\\mathcal{L}_{\\text{data}}$, (eventually) some known data.\n",
    "\n",
    "![PINNs](https://media.springernature.com/full/springer-static/image/art%3A10.1007%2Fs10915-022-01939-z/MediaObjects/10915_2022_1939_Fig2_HTML.png?as=webp)\n",
    "\n",
    "[Source](https://link.springer.com/article/10.1007/s10915-022-01939-z)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PINNs solving ODEs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our Lotka-Volterra system the solution $u$ will be a vector such that \n",
    "$$\n",
    "u(t) = (x(t), y(t))^\\top\n",
    "$$\n",
    "and there are not boundary conditions, only initial conditions.\n",
    "\n",
    "We want to train a network that looks like this\n",
    "\n",
    "![lotka_volterra_pinn](../images/lotka_volterra_pinn.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important package is `deepxde`, which allows us to implement Physic-Informed Neural Networs approaches with a few lines of code. We will need `numpy` for array operations, `matplotlib` for visualizations and `scipy` for getting the real solution (which is not always possible, if not we would only use this approach)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install deepxde  # Run this line if you are in Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import deepxde as dde\n",
    "\n",
    "from scipy.integrate import solve_ivp\n",
    "from deepxde.backend import tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ODE Residuals"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are trying to embed the physics onto the neural networks we need to define "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 2 / 3\n",
    "beta = 4 / 3\n",
    "gamma = 1\n",
    "delta = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ode(t, Y):\n",
    "    x = Y[:, 0:1]\n",
    "    y = Y[:, 1:2]\n",
    "\n",
    "    dx_dt = dde.grad.jacobian(Y, t, i=0)\n",
    "    dy_dt = dde.grad.jacobian(Y, t, i=1)\n",
    "    \n",
    "    return [\n",
    "        dx_dt - alpha * x + beta * x * y,\n",
    "        dy_dt + gamma * y  - delta * x * y\n",
    "    ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `t` is the indepent variable and `Y` is an array with two columns (since our system considers two equations). To define the first derivative is as easy as using `dde.grad.jacobian`, just be sure about the component `i`, in this case we decided `i=0` corresponds to the variable $x(t)$ and `i=1` to $y(t)$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial conditions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to decide a time invertal where we will working on. As an example let's consider between $t=0$ and $t=1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_initial = 0\n",
    "t_final = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we need to declare this element for our neural network, if not, the algorithm wouldn't know where to make the estimations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geom = dde.geometry.TimeDomain(t_initial, t_final)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we have to create a function for defining boundaries, since our geometry it is only on time we will use the default one, don't worry about it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundary(_, on_initial):\n",
    "    return on_initial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's choose initial conditions for $x$ and $y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = 1.2\n",
    "y0 = 0.8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we have to tell to our algorithm these are the initial conditions for the learning process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic_x = dde.icbc.IC(geom, lambda x: x0, boundary, component=0)\n",
    "ic_y = dde.icbc.IC(geom, lambda x: y0, boundary, component=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data object"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything related to the differential equations and initial conditions has to be inside a new object `dde.data.PDE` (do not worry, it also consider systems of ordinary differential equations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dde.data.PDE(\n",
    "    geom,\n",
    "    ode,\n",
    "    [ic_x, ic_y],\n",
    "    num_domain=512,\n",
    "    num_boundary=2\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test our model we need more points, we considered 512 points inside our domain with `num_domain=512`. Finally, since we are working on a time domain there are only two points in its boundary (`num_boundary=2`)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time for choosing a neural network architecture. For simplicity, we will use a Fully-connected neural network (`dde.nn.FNN'). The most important things are:\n",
    "\n",
    "- Input layer (the first one) needs only one node/neuron since our indepent variable is only time $t$.\n",
    "- The output layer (the last one) nneds two nodes/neurons since we are working on a system of two equations.\n",
    "\n",
    "Do not worry so much about the amount of layers or neurons in each hidden layer, as a rule of thumb error should decrease while you add more layers and neurons, but it will take more computational time. Activation functions and the initializer are more parameters the user must choose, usually `Glorot normal` works well as initializer. However, we would recommend you to try different activation functions, for example, `relu`, `sigmoid` or `swish`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons = 64\n",
    "layers = 6\n",
    "activation = \"tanh\"\n",
    "initializer = \"Glorot normal\"\n",
    "net = dde.nn.FNN([1] + [neurons] * layers + [2], activation, initializer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The library we are working with needs put everything together in a new object, but it is just one line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dde.Model(data, net)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training we will go with an Adam optimizer (a very popular one nowadays) and a learning rate of 0.001 (smaller learning rates may give you better results but it will take many more iterations).\n",
    "\n",
    "Just for simplicity we will take 50000 iterations, but another rule of thumb it is that as you increase the number of iterations the loss value should decrease as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\"adam\", lr=0.001)\n",
    "losshistory, train_state = model.train(iterations=50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(\"L-BFGS\")\n",
    "# losshistory, train_state = model.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the loss history with a simple command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dde.utils.external.plot_loss_history(losshistory)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to compare our model. We chose the Lotka-Volterra because we can obtain the solution with a Runge-Kuta algorithm, for instance, we used `solve_ivp` from `scipy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(\n",
    "    t,\n",
    "    x0,\n",
    "    y0,\n",
    "    alpha,\n",
    "    beta,\n",
    "    gamma,\n",
    "    delta\n",
    "):\n",
    "\n",
    "    def func(t, Y):\n",
    "        x, y = Y\n",
    "        dx_dt = alpha * x - beta * x * y\n",
    "        dy_dt = - gamma * y  + delta * x * y\n",
    "        return dx_dt, dy_dt\n",
    "\n",
    "    Y0 = [x0, y0]\n",
    "    t_span = (t[0], t[-1])\n",
    "    sol = solve_ivp(func, t_span, Y0, t_eval=t)\n",
    "    x_true, y_true = sol.y\n",
    "    return x_true, y_true"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a fine grid of time, generate data with Runge-Kuta and then compare both approaches in the same plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(t_initial, t_final, 100)\n",
    "x_true, y_true = generate_data(t, x0, y0, alpha, beta, gamma, delta)\n",
    "plt.plot(t, x_true, color=\"green\", label=\"x_true\")\n",
    "plt.plot(t, y_true, color=\"blue\", label=\"y_true\")\n",
    "\n",
    "sol_pred = model.predict(t.reshape(-1, 1))\n",
    "x_pred = sol_pred[:, 0:1]\n",
    "y_pred = sol_pred[:, 1:2]\n",
    "\n",
    "plt.plot(t, x_pred, color=\"red\", linestyle=\"dashed\", label=\"x_pred\")\n",
    "plt.plot(t, y_pred, color=\"orange\", linestyle=\"dashed\", label=\"y_pred\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see both algorithms gave us almost identical results. One of the pros that we would like to point out about Physics-Informed neural networks is that for more complex systems you only need to change a very few things more, specifically residuals. Most of the numerical work is done automatically by machine learning libraries as `TensorFlow`, `Torch`, `JAX`, etc. so it is easy to scale it up, even better when we can take advantage of GPUs. For you, as an user, your challenge will be in pick suitable hyper-parameters (number of layers, number of neurons, activation function, number of iterations, etc.) but this also could be done by other algorithms, however these are out of the scope of this lesson."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "405c5bcf4c9032db942388c872eee0e913b5e1c707337a4970c84578e3301274"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
